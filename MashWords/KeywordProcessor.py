"""
Enhanced Features:
1. OR condition: Delhi, Mumbai, Mouse (comma-separated)
2. AND condition: Mouse+Cheese (plus-separated)
3. Mixed conditions: Delhi, Packet, Mouse+Cheese
4. Backward compatibility maintained
5. Progressive typing logic preserved

Fixed logic flow:
1. Parse main keywords for OR/AND conditions
2. Generate combinations based on conditions
3. Secondary keywords are combined with main keyword combinations
4. Only the SECONDARY part gets progressive typing
5. Proper deduplication of similar progressive sequences
6. Added compatibility layer for Instagram scraper
"""

import re
from typing import List, Tuple, Dict, Optional, Set
from enum import Enum
from collections import defaultdict
import itertools


class SearchStrategy(Enum):
    STANDARD = "standard"
    PREFIX = "prefix"
    SUFFIX = "suffix"
    COMBINED = "combined"


class KeywordCondition:
    """Helper class to parse and handle keyword conditions"""

    def __init__(self, condition_str: str):
        self.original = condition_str.strip()
        self.type = self._determine_type()
        self.keywords = self._parse_keywords()

    def _determine_type(self) -> str:
        """Determine if this is AND, OR, or SIMPLE condition"""
        if '+' in self.original and ',' in self.original:
            return 'MIXED'  # Both AND and OR
        elif '+' in self.original:
            return 'AND'
        elif ',' in self.original:
            return 'OR'
        else:
            return 'SIMPLE'

    def _parse_keywords(self) -> List[List[str]]:
        """Parse keywords based on condition type"""
        if self.type == 'SIMPLE':
            return [[self.original.strip()]]

        elif self.type == 'AND':
            # Split by + and return as single group
            parts = [part.strip() for part in self.original.split('+') if part.strip()]
            return [parts]

        elif self.type == 'OR':
            # Split by comma, each as separate group
            parts = [part.strip() for part in self.original.split(',') if part.strip()]
            return [[part] for part in parts]

        elif self.type == 'MIXED':
            # Parse mixed conditions: Delhi, Packet, Mouse+Cheese
            or_groups = []
            for part in self.original.split(','):
                part = part.strip()
                if '+' in part:
                    # This is an AND condition
                    and_parts = [p.strip() for p in part.split('+') if p.strip()]
                    or_groups.append(and_parts)
                else:
                    # This is a simple term
                    or_groups.append([part])
            return or_groups

        return [[self.original.strip()]]

    def get_all_combinations(self) -> List[str]:
        """Get all possible string combinations for this condition"""
        combinations = []

        for group in self.keywords:
            if len(group) == 1:
                # Simple keyword or single OR option
                combinations.append(group[0])
            else:
                # AND condition - join with space
                combinations.append(' '.join(group))

        return combinations


class OptimizedKeywordProcessor:
    """
    ENHANCED: Support for OR/AND conditions in main keywords
    """

    def __init__(self, main_keywords: List[str], secondary_keywords: List[str] = None):
        """
        Initialize with main and secondary keywords

        Args:
            main_keywords: Keywords with OR/AND conditions (e.g., ['Delhi, Mumbai', 'Food+Travel'])
            secondary_keywords: Keywords to combine WITH main keywords (e.g., ['Food', 'Travel', 'Fun'])
        """
        self.main_keywords_raw = [kw.strip() for kw in main_keywords if kw.strip()]
        self.secondary_keywords = [kw.strip() for kw in (secondary_keywords or [])]

        # Parse main keywords for conditions
        self.main_conditions = [KeywordCondition(kw) for kw in self.main_keywords_raw]
        self.main_keywords = self._expand_main_keywords()

    def _expand_main_keywords(self) -> List[str]:
        """Expand main keywords based on OR/AND conditions"""
        expanded = []

        for condition in self.main_conditions:
            combinations = condition.get_all_combinations()
            expanded.extend(combinations)

        # Remove duplicates while preserving order
        seen = set()
        unique_expanded = []
        for kw in expanded:
            if kw not in seen:
                seen.add(kw)
                unique_expanded.append(kw)

        return unique_expanded

    def generate_basic_combinations(self) -> List[Dict[str, str]]:
        """
        Step 1-3: Generate basic main+secondary combinations

        Returns:
            List of basic combinations before progressive breakdown
        """
        combinations = []

        # Add standalone main keywords (expanded from conditions)
        for main_kw in self.main_keywords:
            combinations.append({
                'search_term': main_kw,
                'type': 'main_only',
                'main_keyword': main_kw,
                'secondary_keyword': None,
                'pattern': 'standalone'
            })

        # Generate main+secondary combinations
        for main_kw in self.main_keywords:
            for secondary_kw in self.secondary_keywords:

                # Pattern 1: Secondary + Main (e.g., "Food Delhi")
                combinations.append({
                    'search_term': f"{secondary_kw} {main_kw}",
                    'type': 'secondary_main',
                    'main_keyword': main_kw,
                    'secondary_keyword': secondary_kw,
                    'pattern': 'secondary_prefix'
                })

                # Pattern 2: Main + Secondary (e.g., "Delhi Food")
                combinations.append({
                    'search_term': f"{main_kw} {secondary_kw}",
                    'type': 'main_secondary',
                    'main_keyword': main_kw,
                    'secondary_keyword': secondary_kw,
                    'pattern': 'main_prefix'
                })

        return combinations

    def generate_progressive_sequences(self, combination: Dict[str, str]) -> List[Dict[str, str]]:
        """
        Step 4: Generate progressive typing for a combination
        CRITICAL: Only the secondary keyword gets progressive typing

        Args:
            combination: A basic combination to break down progressively

        Returns:
            List of progressive steps
        """
        sequences = []

        if combination['type'] == 'main_only':
            # Main keywords are never typed progressively - return as-is
            sequences.append(combination)
            return sequences

        main_kw = combination['main_keyword']
        secondary_kw = combination['secondary_keyword']
        pattern = combination['pattern']

        # Generate progressive steps for secondary keyword
        for i in range(3, len(secondary_kw) + 1): #Control length from here
            partial_secondary = secondary_kw[:i]

            if pattern == 'secondary_prefix':
                # "F Delhi", "Fo Delhi", "Foo Delhi", "Food Delhi"
                search_term = f"{partial_secondary} {main_kw}"
            else:  # main_prefix
                # "Delhi F", "Delhi Fo", "Delhi Foo", "Delhi Food"
                search_term = f"{main_kw} {partial_secondary}"

            sequences.append({
                'search_term': search_term,
                'type': 'progressive',
                'main_keyword': main_kw,
                'secondary_keyword': secondary_kw,
                'partial_secondary': partial_secondary,
                'pattern': pattern,
                'progress': i / len(secondary_kw),
                'step': i,
                'total_steps': len(secondary_kw)
            })

        return sequences

    def deduplicate_sequences(self, all_sequences: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Step 5: Remove duplicate search terms while preserving meaningful ones

        Args:
            all_sequences: All progressive sequences

        Returns:
            Deduplicated sequences with metadata about conflicts
        """
        # Group by search term
        term_groups = defaultdict(list)
        for seq in all_sequences:
            term_groups[seq['search_term']].append(seq)

        deduplicated = []
        conflict_stats = {
            'total_conflicts': 0,
            'resolved_conflicts': 0,
            'examples': []
        }

        for search_term, sequences in term_groups.items():
            if len(sequences) == 1:
                # No conflict - add as-is
                deduplicated.append(sequences[0])
            else:
                # Conflict detected
                conflict_stats['total_conflicts'] += 1

                # Prioritization logic:
                # 1. Prefer longer partial_secondary (more complete)
                # 2. Prefer secondary_prefix pattern over main_prefix
                # 3. Keep the first occurrence for ties

                best_sequence = max(sequences, key=lambda x: (
                    len(x.get('partial_secondary', '')),  # Longer partial wins
                    x.get('pattern') == 'secondary_prefix',  # Prefer "Food Delhi" over "Delhi Food"
                    -sequences.index(x)  # Earlier occurrence wins (negative for max)
                ))

                deduplicated.append(best_sequence)
                conflict_stats['resolved_conflicts'] += 1

                # Record example
                if len(conflict_stats['examples']) < 3:
                    conflict_info = {
                        'search_term': search_term,
                        'conflicting_sources': [
                            f"{s.get('secondary_keyword', 'N/A')} ({s.get('pattern', 'unknown')})"
                            for s in sequences
                        ],
                        'chosen_source': f"{best_sequence.get('secondary_keyword', 'N/A')} ({best_sequence.get('pattern', 'unknown')})"
                    }
                    conflict_stats['examples'].append(conflict_info)

        # Add conflict metadata
        for seq in deduplicated:
            seq['deduplication_stats'] = conflict_stats

        return deduplicated

    def get_optimized_search_terms(self) -> Tuple[List[str], Dict]:
        """
        Execute the complete optimized logic flow

        Returns:
            Tuple of (search_terms, metadata)
        """
        # Step 1-3: Generate basic combinations
        basic_combinations = self.generate_basic_combinations()

        # Step 4: Generate progressive sequences
        all_sequences = []
        for combination in basic_combinations:
            sequences = self.generate_progressive_sequences(combination)
            all_sequences.extend(sequences)

        # Step 5: Deduplicate
        final_sequences = self.deduplicate_sequences(all_sequences)

        # Extract just the search terms
        search_terms = [seq['search_term'] for seq in final_sequences]

        # Compile metadata
        metadata = {
            'main_keywords_raw': self.main_keywords_raw,
            'main_keywords_expanded': self.main_keywords,
            'condition_analysis': [
                {
                    'original': cond.original,
                    'type': cond.type,
                    'combinations': cond.get_all_combinations()
                }
                for cond in self.main_conditions
            ],
            'basic_combinations_count': len(basic_combinations),
            'total_sequences_before_dedup': len(all_sequences),
            'final_sequences_count': len(final_sequences),
            'deduplication_stats': final_sequences[0]['deduplication_stats'] if final_sequences else {},
            'sequences_detail': final_sequences
        }

        return search_terms, metadata

    def print_detailed_analysis(self):
        """Print comprehensive analysis of the optimization process"""

        print("=" * 80)
        print("ENHANCED KEYWORD PROCESSOR - DETAILED ANALYSIS")
        print("=" * 80)

        print(f"ðŸ“Š CONFIGURATION:")
        print(f"   Main Keywords (Raw): {self.main_keywords_raw}")
        print(f"   Secondary Keywords: {self.secondary_keywords}")

        # Execute the logic flow
        search_terms, metadata = self.get_optimized_search_terms()

        print(f"\nðŸ” CONDITION ANALYSIS:")
        for cond_info in metadata['condition_analysis']:
            print(f"   '{cond_info['original']}' â†’ {cond_info['type']}")
            print(f"     Combinations: {cond_info['combinations']}")

        print(f"\n   Expanded Main Keywords: {metadata['main_keywords_expanded']}")

        print(f"\nðŸ”„ LOGIC FLOW EXECUTION:")
        print(f"   Step 1-3: Generated {metadata['basic_combinations_count']} basic combinations")
        print(f"   Step 4: Expanded to {metadata['total_sequences_before_dedup']} progressive sequences")
        print(f"   Step 5: Deduplicated to {metadata['final_sequences_count']} final terms")

        # Show deduplication stats
        dedup_stats = metadata['deduplication_stats']
        if dedup_stats.get('total_conflicts', 0) > 0:
            print(f"\nðŸ”§ DEDUPLICATION RESULTS:")
            print(f"   Total Conflicts Found: {dedup_stats['total_conflicts']}")
            print(f"   Conflicts Resolved: {dedup_stats['resolved_conflicts']}")

            print(f"\n   ðŸ“ Conflict Examples:")
            for example in dedup_stats.get('examples', []):
                print(f"      Term: '{example['search_term']}'")
                print(f"      Sources: {', '.join(example['conflicting_sources'])}")
                print(f"      Chosen: {example['chosen_source']}")
                print()

        print(f"\nðŸŽ¯ FINAL SEARCH TERMS ({len(search_terms)} total):")
        for i, term in enumerate(search_terms, 1):
            print(f"   {i:2d}. {term}")

        print(f"\nâœ… ENHANCED FEATURES:")
        print(f"   âœ“ OR conditions: Delhi, Mumbai â†’ generates for both")
        print(f"   âœ“ AND conditions: Mouse+Cheese â†’ generates as single term")
        print(f"   âœ“ Mixed conditions: Delhi, Packet, Mouse+Cheese â†’ OR logic")
        print(f"   âœ“ Main keywords never typed progressively")
        print(f"   âœ“ Secondary keywords properly combined with all main variants")
        print(f"   âœ“ Progressive typing only on secondary part")
        print(f"   âœ“ Intelligent deduplication with conflict resolution")

        return search_terms, metadata


# COMPATIBILITY LAYER FOR INSTAGRAM SCRAPER
class KeywordProcessor(OptimizedKeywordProcessor):
    """
    Compatibility wrapper for Instagram scraper
    Maintains the same interface expected by the scraper
    """

    def __init__(self, main_keywords: List[str], min_progressive_length: int = 2,
                 max_progressive_length: int = 12, enable_smart_filtering: bool = True):
        """
        Instagram scraper compatible constructor

        Args:
            main_keywords: Main keywords with OR/AND conditions (preserved)
            min_progressive_length: Minimum length for progressive typing (compatibility)
            max_progressive_length: Maximum length for progressive typing (compatibility)
            enable_smart_filtering: Enable smart filtering (compatibility)
        """
        # Store compatibility parameters
        self.min_progressive_length = min_progressive_length
        self.max_progressive_length = max_progressive_length
        self.enable_smart_filtering = enable_smart_filtering

        # Initialize parent with main keywords (now supporting OR/AND)
        super().__init__(main_keywords=main_keywords, secondary_keywords=[])

        # Track processed keywords for analysis
        self.processed_keywords = []
        self.analysis_data = {}

    def process_keyword_list(self, user_keywords: List[str]) -> List[Dict]:
        """
        Process user keywords and combine with main keywords

        Args:
            user_keywords: User input keywords to process

        Returns:
            List of processed keyword information
        """
        print(f"Processing {len(user_keywords)} user keywords with {len(self.main_keywords_raw)} main keyword conditions...")

        # Set secondary keywords from user input
        self.secondary_keywords = [kw.strip() for kw in user_keywords if kw.strip()]

        # Process each keyword
        processed = []
        for keyword in user_keywords:
            keyword = keyword.strip()
            if keyword:
                # Apply length filtering if enabled
                if self.enable_smart_filtering:
                    if len(keyword) < self.min_progressive_length:
                        print(f"  Skipping '{keyword}' - too short (< {self.min_progressive_length} chars)")
                        continue
                    if len(keyword) > self.max_progressive_length:
                        print(f"  Truncating '{keyword}' - too long (> {self.max_progressive_length} chars)")
                        keyword = keyword[:self.max_progressive_length]

                processed.append({
                    'original': keyword,
                    'processed': keyword,
                    'length': len(keyword),
                    'type': 'user_keyword'
                })

        self.processed_keywords = processed
        print(f"Successfully processed {len(processed)} keywords")
        print(f"Main keywords expanded to: {self.main_keywords}")

        return processed

    def get_all_search_terms(self, user_keywords: List[str]) -> List[str]:
        """
        Get all search terms combining main and user keywords

        Args:
            user_keywords: User input keywords

        Returns:
            List of all search terms
        """
        # Ensure keywords are processed
        if not self.processed_keywords:
            self.process_keyword_list(user_keywords)

        # Update secondary keywords
        self.secondary_keywords = [kw['processed'] for kw in self.processed_keywords]

        # Get optimized search terms
        search_terms, metadata = self.get_optimized_search_terms()

        # Store analysis data
        self.analysis_data = metadata

        return search_terms

    def optimize_search_order(self, search_terms: List[str]) -> List[str]:
        """
        Optimize the order of search terms

        Args:
            search_terms: List of search terms to optimize

        Returns:
            Optimized list of search terms
        """
        print(f"Optimizing search order for {len(search_terms)} terms...")

        # Simple optimization: prioritize shorter terms first (progressive typing)
        # This matches the progressive typing logic
        optimized = sorted(search_terms, key=lambda x: (len(x), x))

        print(f"Search order optimized - progressive typing sequence maintained")
        return optimized

    def print_analysis_report(self, processed_keywords: List[Dict]):
        """
        Print analysis report for Instagram scraper

        Args:
            processed_keywords: Processed keyword data
        """
        print(f"\nðŸ“Š ENHANCED KEYWORD ANALYSIS REPORT")
        print(f"=" * 60)
        print(f"Main Keyword Conditions: {len(self.main_keywords_raw)}")
        print(f"Expanded Main Keywords: {len(self.main_keywords)}")
        print(f"User Keywords: {len(processed_keywords)}")
        print(f"Progressive Length Range: {self.min_progressive_length}-{self.max_progressive_length}")
        print(f"Smart Filtering: {'Enabled' if self.enable_smart_filtering else 'Disabled'}")

        if self.analysis_data:
            print(f"\nCondition Analysis:")
            for cond_info in self.analysis_data.get('condition_analysis', []):
                print(f"  '{cond_info['original']}' â†’ {cond_info['type']} â†’ {len(cond_info['combinations'])} combinations")

            print(f"\nGeneration Statistics:")
            print(f"  Basic Combinations: {self.analysis_data.get('basic_combinations_count', 0)}")
            print(f"  Total Sequences: {self.analysis_data.get('total_sequences_before_dedup', 0)}")
            print(f"  Final Terms: {self.analysis_data.get('final_sequences_count', 0)}")

            dedup_stats = self.analysis_data.get('deduplication_stats', {})
            if dedup_stats.get('total_conflicts', 0) > 0:
                print(f"  Conflicts Resolved: {dedup_stats.get('resolved_conflicts', 0)}")

        print(f"\nâœ… Ready for Instagram scraping with OR/AND support!")


# DEMONSTRATION
if __name__ == "__main__":
    print("ðŸ”¥ DEMONSTRATING ENHANCED OR/AND LOGIC WITH COMPATIBILITY")
    print("=" * 70)

    # Test 1: OR conditions
    print("\n1. Testing OR Conditions:")
    print("-" * 30)
    processor1 = OptimizedKeywordProcessor(
        main_keywords=["Delhi, Mumbai, Kolkata"],
        secondary_keywords=["Food", "Travel"]
    )
    terms1, meta1 = processor1.get_optimized_search_terms()
    print(f"Main: ['Delhi, Mumbai, Kolkata'] + Secondary: ['Food', 'Travel']")
    print(f"Generated {len(terms1)} terms including: {terms1[:6]}...")

    # Test 2: AND conditions
    print("\n2. Testing AND Conditions:")
    print("-" * 30)
    processor2 = OptimizedKeywordProcessor(
        main_keywords=["Mouse+Cheese"],
        secondary_keywords=["Fun"]
    )
    terms2, meta2 = processor2.get_optimized_search_terms()
    print(f"Main: ['Mouse+Cheese'] + Secondary: ['Fun']")
    print(f"Generated {len(terms2)} terms: {terms2}")

    # Test 3: Mixed conditions
    print("\n3. Testing Mixed Conditions:")
    print("-" * 30)
    processor3 = OptimizedKeywordProcessor(
        main_keywords=["Delhi, Packet, Mouse+Cheese"],
        secondary_keywords=["Food"]
    )
    terms3, meta3 = processor3.get_optimized_search_terms()
    print(f"Main: ['Delhi, Packet, Mouse+Cheese'] + Secondary: ['Food']")
    print(f"Generated {len(terms3)} terms: {terms3}")

    # Test 4: Instagram Scraper Compatibility
    print("\n4. Testing Instagram Scraper Compatibility:")
    print("-" * 45)
    main_keywords = ["Delhi, Mumbai", "Travel+Guide"]
    processor = KeywordProcessor(
        main_keywords=main_keywords,
        min_progressive_length=2,
        max_progressive_length=12,
        enable_smart_filtering=True
    )

    user_keywords = ["Food", "Fun"]
    processed = processor.process_keyword_list(user_keywords)
    search_terms = processor.get_all_search_terms(user_keywords)
    optimized = processor.optimize_search_order(search_terms)

    processor.print_analysis_report(processed)

    print(f"\nðŸŽ¯ FINAL OPTIMIZED SEARCH TERMS:")
    for i, term in enumerate(optimized[:15], 1):
        print(f"   {i:2d}. {term}")
    if len(optimized) > 15:
        print(f"   ... and {len(optimized) - 15} more")

    print(f"\nâœ… ENHANCED FEATURES VERIFIED!")
    print(f"âœ“ OR conditions work: Delhi, Mumbai â†’ generates for both")
    print(f"âœ“ AND conditions work: Mouse+Cheese â†’ single combined term")
    print(f"âœ“ Mixed conditions work: Delhi, Packet, Mouse+Cheese")
    print(f"âœ“ Instagram scraper compatibility maintained")
    print(f"âœ“ Progressive typing logic preserved")
    print(f"âœ“ All existing functionality intact")